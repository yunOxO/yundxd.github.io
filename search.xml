<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>1-NLP个人复习大纲</title>
      <link href="/2025/07/13/1-NLP%E4%B8%AA%E4%BA%BA%E5%A4%8D%E4%B9%A0%E5%A4%A7%E7%BA%B2/"/>
      <url>/2025/07/13/1-NLP%E4%B8%AA%E4%BA%BA%E5%A4%8D%E4%B9%A0%E5%A4%A7%E7%BA%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP基础知识整理"><a href="#NLP基础知识整理" class="headerlink" title="NLP基础知识整理"></a>NLP基础知识整理</h1><h2 id="输入-→-预处理"><a href="#输入-→-预处理" class="headerlink" title="输入 → 预处理"></a>输入 → 预处理</h2><h3 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h3><ul><li>粒度划分：word-level、char-level、subword-level</li></ul><p><strong>常见的三种编码方法{BPE、WordPiece、ULM}</strong></p><blockquote><p>BPE 字节对编码</p><blockquote><p>一种数据压缩方法，通过迭代地合并最频繁出现的字符或字符序列来实现分词目的</p></blockquote></blockquote><blockquote><p>WordPiece</p><blockquote><p>WordPiece 是 BPE 的一种变体，不同点：WordPiece 基于概率生成新的subword而不是下一最高频字节对</p></blockquote></blockquote><blockquote><p>ULM</p></blockquote><blockquote><p>SentencePiece</p></blockquote><h3 id="编码（词表征）"><a href="#编码（词表征）" class="headerlink" title="编码（词表征）"></a>编码（词表征）</h3><ul><li><p>one-hot，最最基础的编码方式，词典多大，向量维数多大；</p></li><li><p>word2vec，<strong>词向量模型</strong>，用于将词转换为向量表示;</p><blockquote><p>基于两种不同的训练词向量的方法，又可以分为 CBOW模型 和 skip-Gram模型</p><blockquote><ul><li>CBOW，使用上下文来预测当前词的生成概率</li><li>skip-Gram，使用当前词来预测上下文词的生成概率</li></ul></blockquote></blockquote></li><li><p>Glove，基于全局词频统计，统计的是固定语料信息</p></li><li><p>ELMO，针对多义词在不同语境下，不同语义时的表征作出了处理；</p></li></ul><p>【word2vec则是根据局部语料库训练】<br>【损失函数上的差异：word2vec - 带权重的交叉熵，权重固定；glove - 最小平方损失函数，权重可以做映射变化】</p><h3 id="归一化（normalization）"><a href="#归一化（normalization）" class="headerlink" title="归一化（normalization）"></a>归一化（normalization）</h3><h4 id="batch-normalization（BN）"><a href="#batch-normalization（BN）" class="headerlink" title="batch normalization（BN）"></a>batch normalization（BN）</h4><h4 id="layer-normalization（LN）"><a href="#layer-normalization（LN）" class="headerlink" title="layer normalization（LN）"></a>layer normalization（LN）</h4><p>在NLP中，普遍采用的都是LN，直观上的原因是LN效果更好</p><h2 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h2><pre><code>超级重要的模型框架 --- Transformer  </code></pre><h3 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h3><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><h4 id="结构相关"><a href="#结构相关" class="headerlink" title="结构相关"></a>结构相关</h4><p>| Encoder-Decoder | Encoder | Decoder |</p><h5 id="细分"><a href="#细分" class="headerlink" title="细分"></a>细分</h5><p>Casual Decoder<br>Prefix Decoder<br>相同：都是基于Decoder-only的模型框架；<br>区别：注意力机制的设计不同，Casual Decoder采用单向注意力</p><h3 id="进阶模型"><a href="#进阶模型" class="headerlink" title="进阶模型"></a>进阶模型</h3><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><h4 id="GPT1、2、3"><a href="#GPT1、2、3" class="headerlink" title="GPT1、2、3"></a>GPT1、2、3</h4><p>Embedding -&gt; Masked Mutil Self Attention -&gt; LN -&gt; Feed Forward -&gt; LN -&gt; Prediction + task Classifierpin<br>生成式预训练（无监督） + 判别式任务精调（有监督）</p><h4 id="InstructGPT"><a href="#InstructGPT" class="headerlink" title="InstructGPT"></a>InstructGPT</h4><h4 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h4><h4 id="GPT-4"><a href="#GPT-4" class="headerlink" title="GPT-4"></a>GPT-4</h4><h4 id="BART-（facebook）"><a href="#BART-（facebook）" class="headerlink" title="BART （facebook）"></a>BART （facebook）</h4><p>结合了bert 和 gpt<br>新的训练方法<br>对训练样本加入噪声，模型的任务是重构数据样本；<br>降噪自编码器</p><h3 id="目前的大模型"><a href="#目前的大模型" class="headerlink" title="目前的大模型"></a>目前的大模型</h3><p>（GPT-4目前人类之下唯一真神）</p><h4 id="三大主流开源大型"><a href="#三大主流开源大型" class="headerlink" title="三大主流开源大型"></a>三大主流开源大型</h4><ul><li><p>LLaMA、LLaMA2，出自Meta，</p><blockquote><p>衍生模型</p><blockquote></blockquote></blockquote></li><li><p>ChatGLM，清华大学THUNLP，</p></li><li><p>BLOOM，BigScience团队，22年7月</p></li></ul><h3 id="重点关注的模型：T5、GPT"><a href="#重点关注的模型：T5、GPT" class="headerlink" title="重点关注的模型：T5、GPT"></a>重点关注的模型：T5、GPT</h3><p>T5</p><h3 id="任务相关"><a href="#任务相关" class="headerlink" title="任务相关"></a>任务相关</h3><h4 id="距离特征"><a href="#距离特征" class="headerlink" title="距离特征"></a>距离特征</h4><p>向量是数据建模后的特征值v，特征向量之间的差距常用于用于分类、相似性匹配等任务中</p><ul><li>欧几里得距离，两点之间的直线距离；</li><li>曼哈顿距离，各个坐标系距离之和；|x2-x1| + |y2-y1|【在高维空间中，效果比欧几里得距离更好】</li><li>切比雪夫距离，各坐标系距离中的最大值；max(|x2-x1|,|y2-y1|)</li><li>余弦距离（也即余弦相似性），计算两个n维空间向量之间角度的余弦，取值范围在[-1,1]，-1为完全不相似，1为完全相似；</li></ul><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="优化-梯度下降、计算损失"><a href="#优化-梯度下降、计算损失" class="headerlink" title="优化 { 梯度下降、计算损失 }"></a>优化 { 梯度下降、计算损失 }</h3><h4 id="负采样（Negative-Sampling）"><a href="#负采样（Negative-Sampling）" class="headerlink" title="负采样（Negative Sampling）"></a>负采样（Negative Sampling）</h4><p>context-target为样本，以正确对 orange-juice 作为正样例，控制context，随机选择错误的target作为训练样本，这类错误的训练样本提取过程称为负采样；</p><p>目标：降低训练成本</p><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>不同任务的评估方法各不相同</p><h4 id="摘要任务"><a href="#摘要任务" class="headerlink" title="摘要任务"></a>摘要任务</h4><blockquote><p>ROUGE</p></blockquote><h4 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h4><blockquote><p>acc、准确率、召回率、F1-score</p></blockquote><h3 id="（其它）后处理"><a href="#（其它）后处理" class="headerlink" title="（其它）后处理"></a>（其它）后处理</h3><h4 id="归一化-标准化（Normalization）"><a href="#归一化-标准化（Normalization）" class="headerlink" title="归一化|标准化（Normalization）"></a>归一化|标准化（Normalization）</h4><p>为了方便数据进行处理，将数据转换成(0,1)或(-1,1)之间</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>14-accelerate</title>
      <link href="/2023/11/26/14-accelerate/"/>
      <url>/2023/11/26/14-accelerate/</url>
      
        <content type="html"><![CDATA[<p>关于 Hugging Face开源库accelerate 详解：<a href="https://zhuanlan.zhihu.com/p/646610811">https://zhuanlan.zhihu.com/p/646610811</a></p><p>Huggingface，Accelerate文档介绍：<a href="https://huggingface.co/docs/accelerate/index">https://huggingface.co/docs/accelerate/index</a></p><p>示例介绍：<a href="http://www.360doc.com/content/23/0206/22/7673502_1066492586.shtml">http://www.360doc.com/content/23/0206/22/7673502_1066492586.shtml</a></p><p>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accelerate，33B的LLaMA模型，多卡推理代码</span></span><br><span class="line"><span class="comment"># LLaMA-33B 一般需要66G 显存，每张卡允许使用35G显存空间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> LlamaConfig,LlamaForCausalLM,LlamaTokenizer</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> init_empty_weights,infer_auto_device_map,load_checkpoint_in_model,dispatch_model</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">cuda_list = <span class="string">&#x27;6,7&#x27;</span>.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"><span class="comment"># 显存控制</span></span><br><span class="line">memory = <span class="string">&#x27;35GiB&#x27;</span></span><br><span class="line">model_path = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">no_split_module_classes = LlamaForCausalLM._no_split_modules</span><br><span class="line"></span><br><span class="line">max_memory = &#123;<span class="built_in">int</span>(cuda):memory <span class="keyword">for</span> cuda <span class="keyword">in</span> cuda_list&#125;</span><br><span class="line">config = LlamaConfig.from_pretrained(model_path)</span><br><span class="line"><span class="keyword">with</span> init_empty_weights():</span><br><span class="line">    model = LlamaForCausalLM._from_config(config, torch_dtype=torch.float16) <span class="comment">#加载到meta设备中，不需要耗时，不需要消耗内存和显存</span></span><br><span class="line"></span><br><span class="line">device_map = infer_auto_device_map(model, max_memory=max_memory,no_split_module_classes=no_split_module_classes) <span class="comment">#自动划分每个层的设备</span></span><br><span class="line">load_checkpoint_in_model(model,model_path,device_map=device_map) <span class="comment">#加载权重</span></span><br><span class="line">model = dispatch_model(model,device_map=device_map) <span class="comment">#并分配到具体的设备上</span></span><br><span class="line"></span><br><span class="line">tokenizer = LlamaTokenizer.from_pretrained(model_path)</span><br><span class="line">torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">sents=[<span class="string">&#x27;你是谁&#x27;</span>]</span><br><span class="line">ids = tokenizer(sents,max_length=<span class="number">1800</span>,padding=<span class="literal">True</span>,truncation=<span class="literal">True</span>,return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">ids = ids.to(model.device) </span><br><span class="line">outputs = model.generate(**ids, do_sample=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>13-NLP的一些项目</title>
      <link href="/2023/10/16/13-NLP%E7%9A%84%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE/"/>
      <url>/2023/10/16/13-NLP%E7%9A%84%E4%B8%80%E4%BA%9B%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h1 id="13-NLP的一些项目"><a href="#13-NLP的一些项目" class="headerlink" title="13-NLP的一些项目"></a>13-NLP的一些项目</h1><h2 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h2><ul><li>输入：问题</li><li>语料库</li><li>知识库</li><li>输出： 答案</li></ul><h2 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h2><h3 id="舆情监控"><a href="#舆情监控" class="headerlink" title="舆情监控"></a>舆情监控</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">机器学习： 输入语句 → 特征工程 → 模型 → 情感值</span><br><span class="line">深度学习： 输入语句 → 深度学习模型 → 情感值</span><br></pre></td></tr></table></figure><h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><h2 id="自动摘要"><a href="#自动摘要" class="headerlink" title="自动摘要"></a>自动摘要</h2><h2 id="聊天机器人"><a href="#聊天机器人" class="headerlink" title="聊天机器人"></a>聊天机器人</h2><h2 id="信息提取-Information-Extraction（挺重要的领域）"><a href="#信息提取-Information-Extraction（挺重要的领域）" class="headerlink" title="信息提取 Information Extraction（挺重要的领域）"></a>信息提取 Information Extraction（挺重要的领域）</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>12-论文阅读记录</title>
      <link href="/2023/09/25/12-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/09/25/12-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h1><h2 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h2><h3 id="0-《【模板】标题》"><a href="#0-《【模板】标题》" class="headerlink" title="0 -《【模板】标题》"></a>0 -《【模板】标题》</h3><h6 id="翻译："><a href="#翻译：" class="headerlink" title="翻译："></a>翻译：</h6><ul><li>关键词：<strong>NLP</strong></li><li>数据集：</li><li>方法介绍：</li><li>结论</li><li>实现：</li></ul><h3 id="1-《Annotating-and-Detecting-Fine-grained-Factual-Errors-for-Dialogue-Summarization》"><a href="#1-《Annotating-and-Detecting-Fine-grained-Factual-Errors-for-Dialogue-Summarization》" class="headerlink" title="1 -《Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization》"></a>1 -《Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization》</h3><h6 id="翻译：注释和检测细粒度事实错误以进行对话总结"><a href="#翻译：注释和检测细粒度事实错误以进行对话总结" class="headerlink" title="翻译：注释和检测细粒度事实错误以进行对话总结"></a>翻译：注释和检测细粒度事实错误以进行对话总结</h6><ul><li><p>关键词：<strong>事实错误检测</strong>、<strong>事实错误注释</strong>、<strong>数据集：DIASUMFACT</strong>、<strong>模型方法：ENDERANKER</strong></p><p>  事实错误注释&#x2F;标注，作者使用DIASUMFACT数据集对对话摘要中的细粒度句子级事实错误进行了注释，包括错误类别、错误跨度和解释</p></li></ul><p>事实错误的检测方案：</p><p>1.基于文本蕴含的模型（二分类）；</p><pre><code>优点：有效地检测出事实错误；缺点：需要大量标注数据的训练集</code></pre><p>2.基于问答的模型</p><pre><code>优点：易于纠正和理解缺点：如何生成问题</code></pre><h4 id="模型方法：ENDERANKER"><a href="#模型方法：ENDERANKER" class="headerlink" title="模型方法：ENDERANKER"></a>模型方法：<strong>ENDERANKER</strong></h4><ol><li><p>生成摘要：ENDERANKER使用不同的预训练编码器-解码器模型来生成摘要，包括T5、PEGASUS和BART等。</p></li><li><p>生成依存树：ENDERANKER使用Stanford CoreNLP工具生成摘要的依存树。</p></li><li><p>规则检测：ENDERANKER使用一组规则来检测依存树中的事实错误，包括错误类别、错误跨度和解释等。</p></li><li><p>模型检测：如果规则检测未能检测到错误，ENDERANKER将使用BERTMULTI模型来检测错误。</p></li><li><p>合并结果：ENDERANKER将规则检测和模型检测的结果合并起来，以获得最终的事实错误检测结果。</p></li></ol><p><code>总的来说，ENDERANKER使用了多种技术来检测摘要中的事实错误，包括预训练编码器-解码器模型、依存树分析、规则检测和模型检测等。</code></p><h3 id="2-《Automated-Metrics-for-Medical-Multi-Document-Summarization-Disagree-with-Human-Evaluations》"><a href="#2-《Automated-Metrics-for-Medical-Multi-Document-Summarization-Disagree-with-Human-Evaluations》" class="headerlink" title="2 -《Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations》"></a>2 -《Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations》</h3><h3 id="3-《Cross-lingual-Science-Journalism-SELECT-SIMPLIFY-and-REWRITE-Summaries-for-Non-expert-Readers》"><a href="#3-《Cross-lingual-Science-Journalism-SELECT-SIMPLIFY-and-REWRITE-Summaries-for-Non-expert-Readers》" class="headerlink" title="3 -《Cross-lingual Science Journalism: SELECT, SIMPLIFY and REWRITE Summaries for Non-expert Readers》"></a>3 -《Cross-lingual Science Journalism: SELECT, SIMPLIFY and REWRITE Summaries for Non-expert Readers》</h3><p>关键词：<strong>SSR框架</strong>、三个组件-SEKECT选候选摘要句子、SIMPLIFY生成一个语言的简单文本、REWRITE重写（采用的是mbart）、</p><hr><h2 id="九月"><a href="#九月" class="headerlink" title="九月"></a>九月</h2><h3 id="《Pretraining-on-the-Test-Set-Is-All-You-Need》"><a href="#《Pretraining-on-the-Test-Set-Is-All-You-Need》" class="headerlink" title="《Pretraining on the Test Set Is All You Need》"></a>《Pretraining on the Test Set Is All You Need》</h3><p>会议：？</p><p>时间：2023-09</p><p>link：<a href="https://arxiv.org/pdf/2309.08632.pdf">https://arxiv.org/pdf/2309.08632.pdf</a></p><p>借助了一个高质量数据集（由huggingface上评估基准的数据集），来训练一个基于Transformer框架的模型，名为 <strong>phi-CTNL</strong> 的模型。</p><h3 id="Summarization-is-Almost-Dead"><a href="#Summarization-is-Almost-Dead" class="headerlink" title="Summarization is (Almost) Dead"></a>Summarization is (Almost) Dead</h3><p>会议：</p><p>时间：2023-09</p><p>link：<a href="https://arxiv.org/pdf/2309.09558.pdf">https://arxiv.org/pdf/2309.09558.pdf</a></p><p>使用LLMs模型，增对5个摘要任务</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>11-算法练习记录</title>
      <link href="/2023/09/13/11-%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
      <url>/2023/09/13/11-%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="手撕代码热门题"><a href="#手撕代码热门题" class="headerlink" title="手撕代码热门题"></a>手撕代码热门题</h1><h3 id="①-牛客-·-BM6-判断链表中是否有环【简单】"><a href="#①-牛客-·-BM6-判断链表中是否有环【简单】" class="headerlink" title="① 牛客 · BM6 判断链表中是否有环【简单】"></a>① 牛客 · BM6 判断链表中是否有环【简单】</h3><ul><li>方法1：双指针（快、慢指针）【时间复杂度：O(n)、空间复杂度：O(1)】</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasCycle</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        <span class="comment"># 快慢指针</span></span><br><span class="line">        slow = head</span><br><span class="line">        fast = head</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> slow <span class="keyword">and</span> fast:</span><br><span class="line">            slow = slow.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">if</span> fast.<span class="built_in">next</span>:</span><br><span class="line">                fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> slow == fast:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><ul><li>方法2：哈希表（存节点，判断是否有环）【时间复杂度：O(n)、空间复杂度：O(n)】</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasCycle</span>(<span class="params">self, head: ListNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        visited = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">while</span> head:</span><br><span class="line">            <span class="keyword">if</span> head <span class="keyword">in</span> visited:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            visited.add(head)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h3 id="②-牛客-·-BM1-反转链表【简单】"><a href="#②-牛客-·-BM1-反转链表【简单】" class="headerlink" title="② 牛客 · BM1 反转链表【简单】"></a>② 牛客 · BM1 反转链表【简单】</h3><ul><li>方法1：栈实现</li><li>方法2：双链表</li></ul><h3 id="③-合并两个有序链表【简单】"><a href="#③-合并两个有序链表【简单】" class="headerlink" title="③ 合并两个有序链表【简单】"></a>③ 合并两个有序链表【简单】</h3><ul><li>方法1：双指针</li></ul><h3 id="④-二叉树遍历"><a href="#④-二叉树遍历" class="headerlink" title="④ 二叉树遍历"></a>④ 二叉树遍历</h3><ul><li>方法1：递归（深度优先）</li><li>方法2：列表（广度优先）</li></ul><h3 id="⑤-单利模式"><a href="#⑤-单利模式" class="headerlink" title="⑤ 单利模式"></a>⑤ 单利模式</h3><ul><li>方法1：</li></ul><h3 id="⑥-LRU缓存"><a href="#⑥-LRU缓存" class="headerlink" title="⑥ LRU缓存"></a>⑥ LRU缓存</h3><ul><li>方法1：（最优实现）双向链表 + 哈希</li></ul><h3 id="⑦-合并有序数组"><a href="#⑦-合并有序数组" class="headerlink" title="⑦ 合并有序数组"></a>⑦ 合并有序数组</h3><ul><li>方法1：双指针</li></ul><h3 id="⑧-快排"><a href="#⑧-快排" class="headerlink" title="⑧ 快排"></a>⑧ 快排</h3><h3 id="⑨-两数之和"><a href="#⑨-两数之和" class="headerlink" title="⑨ 两数之和"></a>⑨ 两数之和</h3><ul><li>方法1：哈希<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hashset = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">    <span class="keyword">if</span> target - nums[i] <span class="keyword">in</span> hashset:</span><br><span class="line">        <span class="keyword">return</span> [hashset[target-nums[i]], i]</span><br><span class="line">    hashset[nums[i]] = i</span><br><span class="line"><span class="keyword">return</span> []</span><br></pre></td></tr></table></figure></li></ul><h3 id="⑩-最长回文字串"><a href="#⑩-最长回文字串" class="headerlink" title="⑩ 最长回文字串"></a>⑩ 最长回文字串</h3><ul><li>方法1：贪心算法</li></ul><h3 id="⑪-二叉树最近公共祖先"><a href="#⑪-二叉树最近公共祖先" class="headerlink" title="⑪ 二叉树最近公共祖先"></a>⑪ 二叉树最近公共祖先</h3><p>核心思路：遍历</p><h3 id="（其他）判断回文字符串"><a href="#（其他）判断回文字符串" class="headerlink" title="（其他）判断回文字符串"></a>（其他）判断回文字符串</h3><h3 id="（其他）两段字符创的最大公共字符串"><a href="#（其他）两段字符创的最大公共字符串" class="headerlink" title="（其他）两段字符创的最大公共字符串"></a>（其他）两段字符创的最大公共字符串</h3><h3 id="（其他）有序链表转换二叉搜索树"><a href="#（其他）有序链表转换二叉搜索树" class="headerlink" title="（其他）有序链表转换二叉搜索树"></a>（其他）有序链表转换二叉搜索树</h3><h3 id="（其他）两个栈实现队列"><a href="#（其他）两个栈实现队列" class="headerlink" title="（其他）两个栈实现队列"></a>（其他）两个栈实现队列</h3><h3 id="（其他）三个小城交替打印ABC"><a href="#（其他）三个小城交替打印ABC" class="headerlink" title="（其他）三个小城交替打印ABC"></a>（其他）三个小城交替打印ABC</h3><h3 id="（其他）判断回文字符串-1"><a href="#（其他）判断回文字符串-1" class="headerlink" title="（其他）判断回文字符串"></a>（其他）判断回文字符串</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>10-Cpp</title>
      <link href="/2023/09/11/10-Cpp/"/>
      <url>/2023/09/11/10-Cpp/</url>
      
        <content type="html"><![CDATA[<h1 id="C-学习笔记"><a href="#C-学习笔记" class="headerlink" title="C++ 学习笔记"></a>C++ 学习笔记</h1><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>9-java</title>
      <link href="/2023/09/11/9-java/"/>
      <url>/2023/09/11/9-java/</url>
      
        <content type="html"><![CDATA[<h1 id="9-JAVA"><a href="#9-JAVA" class="headerlink" title="9-JAVA"></a>9-JAVA</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>8-python</title>
      <link href="/2023/09/11/8-python/"/>
      <url>/2023/09/11/8-python/</url>
      
        <content type="html"><![CDATA[<h3 id="argparse-模块"><a href="#argparse-模块" class="headerlink" title="argparse 模块"></a>argparse 模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>7-spark笔记</title>
      <link href="/2023/09/07/7-spark%E7%AC%94%E8%AE%B0/"/>
      <url>/2023/09/07/7-spark%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="7-Spark笔记"><a href="#7-Spark笔记" class="headerlink" title="7-Spark笔记"></a>7-Spark笔记</h1><h6 id="基本概念、核心原理、核心源码"><a href="#基本概念、核心原理、核心源码" class="headerlink" title="基本概念、核心原理、核心源码"></a>基本概念、核心原理、核心源码</h6><h6 id="Spark常见报错的解决方案"><a href="#Spark常见报错的解决方案" class="headerlink" title="Spark常见报错的解决方案"></a>Spark常见报错的解决方案</h6><h6 id="Spark参数调优与Spark-SQL调优"><a href="#Spark参数调优与Spark-SQL调优" class="headerlink" title="Spark参数调优与Spark SQL调优"></a>Spark参数调优与Spark SQL调优</h6><h6 id="SPark-SQL开发（写SQL）"><a href="#SPark-SQL开发（写SQL）" class="headerlink" title="SPark SQL开发（写SQL）"></a>SPark SQL开发（写SQL）</h6><h6 id="Spark-企业使用场景的解决方案（UDF管理、Thrift-Server的使用）"><a href="#Spark-企业使用场景的解决方案（UDF管理、Thrift-Server的使用）" class="headerlink" title="Spark 企业使用场景的解决方案（UDF管理、Thrift Server的使用）"></a>Spark 企业使用场景的解决方案（UDF管理、Thrift Server的使用）</h6><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h2 id="Spark—分布式计算框架，与Mapreduce一样，被用于完成数据计算Spark是基于内存计算，"><a href="#Spark—分布式计算框架，与Mapreduce一样，被用于完成数据计算Spark是基于内存计算，" class="headerlink" title="Spark—分布式计算框架，与Mapreduce一样，被用于完成数据计算Spark是基于内存计算，##  "></a>Spark—分布式计算框架，与Mapreduce一样，被用于完成数据计算<br>Spark是基于内存计算，<br>##  </h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>6-研究方向与现状</title>
      <link href="/2023/09/05/6-%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B8%8E%E7%8E%B0%E7%8A%B6/"/>
      <url>/2023/09/05/6-%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B8%8E%E7%8E%B0%E7%8A%B6/</url>
      
        <content type="html"><![CDATA[<p>【※ - 比较在意的point】</p><h1 id="怀着问题学习大模型"><a href="#怀着问题学习大模型" class="headerlink" title="怀着问题学习大模型"></a>怀着问题学习大模型</h1><h2 id="1-什么是大模型（基础理论）？"><a href="#1-什么是大模型（基础理论）？" class="headerlink" title="1. 什么是大模型（基础理论）？"></a>1. 什么是大模型（基础理论）？</h2><h6 id="怎么训练模型？如何掌握训练大模型的规律？"><a href="#怎么训练模型？如何掌握训练大模型的规律？" class="headerlink" title="怎么训练模型？如何掌握训练大模型的规律？"></a>怎么训练模型？如何掌握训练大模型的规律？</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 数据如何准备和组合？</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 如何寻找最优训练配置？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3. 如何阈值下游任务的性能？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h3><h2 id="2-是否需要寻找新的网络框架，以更好地支持大模型的性能提升？"><a href="#2-是否需要寻找新的网络框架，以更好地支持大模型的性能提升？" class="headerlink" title="2. 是否需要寻找新的网络框架，以更好地支持大模型的性能提升？"></a>2. 是否需要寻找新的网络框架，以更好地支持大模型的性能提升？</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 从其他学科中寻找灵感，如何更高效地学习和推理</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-1"><a href="#现状-1" class="headerlink" title="现状"></a>现状</h3><ul><li>QGPT</li></ul><h2 id="3-大模型的高效计算问题"><a href="#3-大模型的高效计算问题" class="headerlink" title="3. 大模型的高效计算问题"></a>3. 大模型的高效计算问题</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何根据硬件资源条件自动选择最合适的优化策略组合以进一步提高模型训练和推理的效率</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-2"><a href="#现状-2" class="headerlink" title="现状"></a>现状</h3><h2 id="4-大模型的高效适配问题"><a href="#4-大模型的高效适配问题" class="headerlink" title="4. 大模型的高效适配问题"></a>4. 大模型的高效适配问题</h2><h6 id="关键词：prompt、finetune"><a href="#关键词：prompt、finetune" class="headerlink" title="关键词：prompt、finetune"></a>关键词：prompt、finetune</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何通过提示学习提高大模型适配下游任务的效率？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 如何利用参数高效微调节约大模型适配的存储和计算成本？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-3"><a href="#现状-3" class="headerlink" title="现状"></a>现状</h3><ul><li>COT</li><li>LORA</li></ul><h2 id="5-大模型的可控生成问题"><a href="#5-大模型的可控生成问题" class="headerlink" title="5. 大模型的可控生成问题"></a>5. 大模型的可控生成问题</h2><h6 id="关键词：生成可控"><a href="#关键词：生成可控" class="headerlink" title="关键词：生成可控"></a>关键词：生成可控</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何提高大模型的可解释性以便人们能够更好地理解和信任它们生成的文本？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-4"><a href="#现状-4" class="headerlink" title="现状"></a>现状</h3><h2 id="6-大模型的安全伦理问题"><a href="#6-大模型的安全伦理问题" class="headerlink" title="6. 大模型的安全伦理问题"></a>6. 大模型的安全伦理问题</h2><h6 id="关键词：隐私、安全"><a href="#关键词：隐私、安全" class="headerlink" title="关键词：隐私、安全"></a>关键词：隐私、安全</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何防止大模型被用于制造恶意软件或进行网络攻击从而保障网络安全？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 如何制定合适的规范和标准来保证大模型的开发和使用符合伦理原则便面产生不良的影响？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-5"><a href="#现状-5" class="headerlink" title="现状"></a>现状</h3><h2 id="※-7-大模型的认知学习问题"><a href="#※-7-大模型的认知学习问题" class="headerlink" title="※ 7. 大模型的认知学习问题"></a>※ 7. 大模型的认知学习问题</h2><h6 id="关键词：特定领域、模型落地"><a href="#关键词：特定领域、模型落地" class="headerlink" title="关键词：特定领域、模型落地"></a>关键词：特定领域、模型落地</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何让大模型学会特定领域内的专业知识从而在该领域内更好地应用？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-6"><a href="#现状-6" class="headerlink" title="现状"></a>现状</h3><ul><li>医疗</li><li>法律</li></ul><h3 id="疑惑"><a href="#疑惑" class="headerlink" title="疑惑"></a>疑惑</h3><ul><li>如何训练？</li></ul><h2 id="8-大模型的创新应用问题"><a href="#8-大模型的创新应用问题" class="headerlink" title="8. 大模型的创新应用问题"></a>8. 大模型的创新应用问题</h2><h6 id="关键词：结合工具"><a href="#关键词：结合工具" class="headerlink" title="关键词：结合工具"></a>关键词：结合工具</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 将大模型与其他技术或工具相结合以实现更高效、更智能的应用</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-7"><a href="#现状-7" class="headerlink" title="现状"></a>现状</h3><ul><li>chatglm3：结合了搜索工具</li></ul><h3 id="疑惑-1"><a href="#疑惑-1" class="headerlink" title="疑惑"></a>疑惑</h3><ul><li>如何结合？</li></ul><h2 id="9-大模型的数据和评估问题"><a href="#9-大模型的数据和评估问题" class="headerlink" title="9. 大模型的数据和评估问题"></a>9. 大模型的数据和评估问题</h2><h6 id="关键词：数据、评估"><a href="#关键词：数据、评估" class="headerlink" title="关键词：数据、评估"></a>关键词：数据、评估</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何在大规模数据上训练和验证大模型以确保其性能和泛化能力？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-8"><a href="#现状-8" class="headerlink" title="现状"></a>现状</h3><h3 id="疑惑-2"><a href="#疑惑-2" class="headerlink" title="疑惑"></a>疑惑</h3><h2 id="※-10-大模型的易用性问题"><a href="#※-10-大模型的易用性问题" class="headerlink" title="※ 10. 大模型的易用性问题"></a>※ 10. 大模型的易用性问题</h2><h6 id="关键词：简化部署和训练过程"><a href="#关键词：简化部署和训练过程" class="headerlink" title="关键词：简化部署和训练过程"></a>关键词：简化部署和训练过程</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">例如</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 如何快速落地？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 如何为大模型提供合适的预训练任务和微调方案以加速用户对大模型的应用和开发过程？</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">...</span></span><br></pre></td></tr></table></figure><h3 id="现状-9"><a href="#现状-9" class="headerlink" title="现状"></a>现状</h3><ul><li>chatglm3：结合了搜索工具</li></ul><h3 id="疑惑-3"><a href="#疑惑-3" class="headerlink" title="疑惑"></a>疑惑</h3><ul><li>如何结合？</li></ul><h1 id="AIGC"><a href="#AIGC" class="headerlink" title="AIGC"></a>AIGC</h1><ol><li>拥有扎实的传统NLP知识基础，能持续维护和持续优化模型，并积极跟进最新的大型语言模型技术，如LLaMa2、LORA等；</li><li>熟悉Prompt Engineering技巧（基于ChatGPT&#x2F;GPT4&#x2F;LLaMA），能够有效地设计和迭代prompt来提升效果；</li><li>擅长进行文本数据处理、特征工程、模型训练和优化；熟悉常用的NLP工具和库；</li><li>具有强烈的责任心，能够独立地负责并推进项目，确保其从起始到结束的顺利进行；</li></ol><hr><ol><li>作为项目主要负责人，进行国际化产品核心功能的早期建设，重新定义招聘领域的co-pilot 体验；</li><li>微调大型语言模型 (LLaMA)，持续增强公司的基础NLP能力；</li><li>与推荐与搜索团 和工作描述开发针对性的NLP特性，确保推荐和搜索质量持续提高；</li><li>能够以技术策略驱动业务增长，进行效果跟踪、数据分析，并及时优化，挖掘业务和系统空间；</li><li>探索AIGC应用方向研究前沿，实现关键技术突破与落地；</li></ol><h2 id="用ChatGPT在推荐领域上，"><a href="#用ChatGPT在推荐领域上，" class="headerlink" title="用ChatGPT在推荐领域上，"></a>用ChatGPT在推荐领域上，</h2><h3 id="rating-prediction-评分预测"><a href="#rating-prediction-评分预测" class="headerlink" title="rating prediction 评分预测"></a>rating prediction 评分预测</h3><h3 id="sequential-recommendation-顺序推荐"><a href="#sequential-recommendation-顺序推荐" class="headerlink" title="sequential recommendation 顺序推荐"></a>sequential recommendation 顺序推荐</h3><h3 id="direct-recommendation-直接推荐"><a href="#direct-recommendation-直接推荐" class="headerlink" title="direct recommendation 直接推荐"></a>direct recommendation 直接推荐</h3><h3 id="explanation-generation-解释生成"><a href="#explanation-generation-解释生成" class="headerlink" title="explanation generation 解释生成"></a>explanation generation 解释生成</h3><h3 id="review-summarization-回复总结"><a href="#review-summarization-回复总结" class="headerlink" title="review summarization 回复总结"></a>review summarization 回复总结</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>5-基础知识</title>
      <link href="/2023/09/04/5-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/09/04/5-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h1 id="5-基础知识"><a href="#5-基础知识" class="headerlink" title="5 - 基础知识"></a>5 - 基础知识</h1><h2 id="5-1-五大常用算法"><a href="#5-1-五大常用算法" class="headerlink" title="5.1 - 五大常用算法"></a>5.1 - 五大常用算法</h2><h3 id="5-1-1-分治法"><a href="#5-1-1-分治法" class="headerlink" title="5.1.1 分治法"></a>5.1.1 分治法</h3><h3 id="5-1-2-动态规划"><a href="#5-1-2-动态规划" class="headerlink" title="5.1.2 动态规划"></a>5.1.2 动态规划</h3><h3 id="5-1-3-贪心"><a href="#5-1-3-贪心" class="headerlink" title="5.1.3 贪心"></a>5.1.3 贪心</h3><h3 id="5-1-4-回溯"><a href="#5-1-4-回溯" class="headerlink" title="5.1.4 回溯"></a>5.1.4 回溯</h3><h3 id="5-1-5-分支限界"><a href="#5-1-5-分支限界" class="headerlink" title="5.1.5 分支限界"></a>5.1.5 分支限界</h3><h2 id="软件开发"><a href="#软件开发" class="headerlink" title="软件开发"></a>软件开发</h2><h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><table><thead><tr><th align="center">排序算法</th><th align="center">平均时间</th><th align="center">最差情况</th><th align="center">稳定程度</th><th align="center">额外空间</th><th align="center">备注</th></tr></thead><tbody><tr><td align="center">冒泡排序</td><td align="center">$O(n^2)$</td><td align="center">$O(n^2)$</td><td align="center">稳定</td><td align="center">$O(1)$</td><td align="center"></td></tr><tr><td align="center">交换排序</td><td align="center">$O(n^2)$</td><td align="center">$O(n^2)$</td><td align="center">不稳定</td><td align="center">$O(1)$</td><td align="center"></td></tr><tr><td align="center">选择排序</td><td align="center">$O(n^2)$</td><td align="center">$O(n^2)$</td><td align="center">不稳定</td><td align="center">$O(1)$</td><td align="center"></td></tr><tr><td align="center">插入排序</td><td align="center">$O(n^2)$</td><td align="center">$O(n^2)$</td><td align="center">稳定</td><td align="center">$O(1)$</td><td align="center"></td></tr><tr><td align="center">快速排序</td><td align="center">$O(nlog(n))$</td><td align="center">$O(n^2)$</td><td align="center">不稳定</td><td align="center">$O(nlog(n))$</td><td align="center"></td></tr><tr><td align="center">希尔排序</td><td align="center">$O(log_2(n))$</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center"></td></tr><tr><td align="center">归并排序</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center"></td></tr><tr><td align="center">堆排序</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center"></td></tr><tr><td align="center">基数排序</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center">单元格</td><td align="center"></td></tr></tbody></table><h4 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h4><h4 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h4><h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h4 id="哈希查找（hash，散列表）"><a href="#哈希查找（hash，散列表）" class="headerlink" title="哈希查找（hash，散列表）"></a>哈希查找（hash，散列表）</h4><p>定义：</p><p>负载因子：散列表的一个重要参数是负载因子a</p><pre><code>a = 散列表中结点的数目/基本区域能容纳的结点数。</code></pre><h2 id="常用的数据结构框架"><a href="#常用的数据结构框架" class="headerlink" title="常用的数据结构框架"></a>常用的数据结构框架</h2><h3 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h3><p>并查集是一种树型的数据结构，用于处理一些不相交集合的合并与查询问题。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>4-大数据分析</title>
      <link href="/2023/08/28/4-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
      <url>/2023/08/28/4-%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据分析相关学习"><a href="#大数据分析相关学习" class="headerlink" title="大数据分析相关学习"></a>大数据分析相关学习</h1><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p><a href="https://blog.csdn.net/qq_20042935/article/details/125536640?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169303762716800225533621%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=169303762716800225533621&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125536640-null-null.142%5Ev93%5EchatsearchT3_2&amp;utm_term=spark&amp;spm=1018.2226.3001.4187">https://blog.csdn.net/qq_20042935/article/details/125536640?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169303762716800225533621%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=169303762716800225533621&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-125536640-null-null.142^v93^chatsearchT3_2&amp;utm_term=spark&amp;spm=1018.2226.3001.4187</a></p><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>3-常用指令集</title>
      <link href="/2023/08/27/3-%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E9%9B%86/"/>
      <url>/2023/08/27/3-%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h1 id="第三篇-常用指令集"><a href="#第三篇-常用指令集" class="headerlink" title="第三篇 - 常用指令集"></a>第三篇 - 常用指令集</h1><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cp，复制文件或目录的指令</span><br><span class="line">    cp [-abdfilpPrRsuvx][-S &lt;备份字尾字符串&gt;][-V &lt;备份方式&gt;][--help][--spares=&lt;使用时机&gt;][--version][源文件或目录][目标文件或目录] [目的目录]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    将file1复制成file2：cp file1 file2</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="hexo-常用指令"><a href="#hexo-常用指令" class="headerlink" title="hexo 常用指令"></a>hexo 常用指令</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">本地预览</span></span><br><span class="line">hexo s   |   hexo server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据配置文件和主题，将 root/source/_post 下的 markdown 文件生成 html 内容</span></span><br><span class="line">hexo g   |   hexo generation </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清空 hexo g 生成的内容，内容在 root/public</span> </span><br><span class="line">hexo clean</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将本地生成 url 链接推送到百度，让百度爬取其中的内容进行索引</span></span><br><span class="line">hexo d   |   hexo deploy</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">根据 标题名称  在root/source/_post 生成 markdown 文件</span></span><br><span class="line">hexo new post 标题名称</span><br></pre></td></tr></table></figure><h2 id="pip国内镜像源"><a href="#pip国内镜像源" class="headerlink" title="pip国内镜像源"></a>pip国内镜像源</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清华源（优先</span></span><br><span class="line">https://pypi.tuna.tsinghua.edu.cn/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">阿里云</span></span><br><span class="line">https://mirrors.aliyun.com/pypi/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">腾讯源</span></span><br><span class="line">https://mirrors.cloud.tencent.com/pypi/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">网易</span></span><br><span class="line">https://mirrors.163.com/pypi/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">豆瓣</span></span><br><span class="line">https://pypi.douban.com/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">百度云</span></span><br><span class="line">https://mirror.baidu.com/pypi/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">华为云</span></span><br><span class="line">https://mirrors.huaweicloud.com/repository/pypi/simple/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">中科大</span></span><br><span class="line">https://pypi.mirrors.ustc.edu.cn/simple/</span><br></pre></td></tr></table></figure><p><strong>修改镜像源的配置文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Linux</span></span><br><span class="line">sudo vi ~/.pip/pip.conf</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">文件中添加一下内容[这里修改伪清华源]</span></span><br><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Windows</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在文件上的路径栏中输入：</span></span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">APPDATA%</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找pip目录，不存在就新建，并在该目录下创建pip.ini文件，并在pip.ini中保存一下内容</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">extra-index-url 为配置多个镜像源</span></span><br><span class="line">[global]</span><br><span class="line">index-url = https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">extra-index-url =</span><br><span class="line">    https://mirrors.aliyun.com/pypi/simple/</span><br><span class="line">    https://mirror.baidu.com/pypi/simple</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2-NLP面试准备</title>
      <link href="/2023/08/27/2-NLP%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/"/>
      <url>/2023/08/27/2-NLP%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<h1 id="科大讯飞—最新咨询"><a href="#科大讯飞—最新咨询" class="headerlink" title="科大讯飞—最新咨询"></a>科大讯飞—最新咨询</h1><p><strong>讯飞星火</strong> —&gt; <strong>讯飞GPT API</strong> 提供500多种常用功能</p><blockquote><p>SuperCLUE中文大模型榜单 - 2023年7月<br>总分排名 6（v1.5版本）（8月15日更新了v2.0版本</p><blockquote></blockquote></blockquote><p>GPT-3 → ChatGPT（GPT-3.5）<br>针对了对话任务任务做了特化，基于人类反馈的强化学习（RLHF），针对人类的</p><p>T5 google</p><p>bart facebook</p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
